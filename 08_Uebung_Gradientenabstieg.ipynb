{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13990fde-efbd-40db-b89a-3a6c90938b5c",
   "metadata": {},
   "source": [
    "# Praxisübung: Logistische Regression und Gradientenabstieg\n",
    "\n",
    "In dieser Übung trainieren Sie ein kleines logistisches Regressionsmodell mittels Gradientenabstieg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9effe1f6-5092-433f-992e-e304e2f1464d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2baa29ef-14a1-4086-a00a-a0653164b859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = \n",
      "[[ 1.  -1. ]\n",
      " [ 1.  -0.8]\n",
      " [ 1.  -0.6]\n",
      " [ 1.  -0.4]\n",
      " [ 1.  -0.2]\n",
      " [ 1.   0. ]\n",
      " [ 1.   0.2]\n",
      " [ 1.   0.4]\n",
      " [ 1.   0.6]\n",
      " [ 1.   0.8]\n",
      " [ 1.   1. ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x19757465e80>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Trainingsdatensatz. Dieser ist im folgenden fest, d.h. die Verlustfunktion bezieht sich \n",
    "# auf einen festen Trainingsdatensatz und wir fassen sie nur noch als Funktion von w auf.\n",
    "\n",
    "# 2 Features: die erste Spalte sorgt dafür, dass der Koeffizienten w_0 (\"Intercept\") mitgeschätzt wird\n",
    "X = np.array([[1,1,1,1,1,1,1,1,1,1,1],\n",
    "              [-1,-0.8,-0.6,-0.4,-0.2,0,0.2,0.4,0.6,0.8,1]]).T\n",
    "y = np.array([0,0,0,0,0,0,1,1,1,1,1])\n",
    "\n",
    "print(f\"X = \\n{X}\")\n",
    "plt.plot(X[:,1],y, \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "650a7471-6b87-45fb-b887-c4f94a2e8c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistische Funktion\n",
    "def logistic(w, X):\n",
    "    return 1/(1+np.exp(-X@w))\n",
    "\n",
    "# Kreuzentropie Funktion\n",
    "def L(w, X=X, y=y):\n",
    "    y_hat = logistic(w, X)\n",
    "    return -np.mean(y*np.log(y_hat) + (1-y)*np.log(1-y_hat))\n",
    "\n",
    "# Ableitung der Kreuzentropie nach w\n",
    "def dLdw(w, X=X, y=y):\n",
    "    y_hat = logistic(w, X)\n",
    "    # Elementweise Multiplikation und spaltenweiser Mittelwert ergibt Gradientenvektor\n",
    "    return np.mean((y_hat-y)[:,np.newaxis]*X, axis=0)\n",
    "\n",
    "# Algorithmus: Gradientenabstieg zur Bestimmung der Parameter w\n",
    "# w0: Startwert/-schätzung für w\n",
    "# alpha: Schrittweite\n",
    "# n_iter: Anzahl Iterationen\n",
    "# Return: w (die Koeffizienten, die L auf den Trainingsdaten minimieren)\n",
    "def gradient_descent(w0, alpha, n_iter):\n",
    "    w = w0\n",
    "    for k in range(n_iter):\n",
    "        # Gib Zielfunktionswert in jeder 1000. Iteration aus\n",
    "        if k % 1000 == 0:\n",
    "            print(L(w))\n",
    "        w = w - alpha * dLdw(w)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adecd512-8a97-4c93-91ac-941be4fdaaa3",
   "metadata": {},
   "source": [
    "## ✏ Aufgabe 1\n",
    "Lassen Sie das Gradientenverfahren mit Schrittweite 1 für 10000 Iterationen laufen. \n",
    "Geben Sie am Ende die gelernten Koeffizienten aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cad28b69-876e-45cb-9557-c5c51bffdaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd19cbde-c19e-484b-8688-ebd25bce1d04",
   "metadata": {},
   "source": [
    "## ✏ Aufgabe 2\n",
    "Sagen Sie die Wahrscheinlichkeit und die Klasse (Schwellwert 0.5) für die Trainingsdaten vorher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16973709-2943-46b0-8e75-29462b54ee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6949ea-26b0-4b44-a859-9d0ae0145c12",
   "metadata": {},
   "source": [
    "## ✏ Aufgabe 3\n",
    "Trainieren Sie ein logistisches Regressionsmodell mit scikit-learn für denselben Datensatz. Was bedeutet der Parameter ``penalty=\"none\"``? Lassen Sie sich auch hier die Koeffizienten ausgeben und sagen Sie Wahrscheinlichkeiten und Klasse für die Trainingsdaten vorher. Beobachten Sie Unterschiede zum Modell oben?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45e90822-e9e1-41d6-92df-6ddcfaa88a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gegenprobe mit scikit-learn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "m = LogisticRegression(fit_intercept=False, penalty=\"none\")\n",
    "\n",
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
